# A simple linear regression algorithm in MAMBA, the MPC language of
# the SCALE-MAMBA library.
#
# The algorithm is an adapted verison of the one presented here:
# https://machinelearningmastery.com/implement-simple-linear-regression-scratch-python/

# Author: Florian Apfelbeck, July, 2018

from Compiler import mpc_math, floatingpoint
from random import seed, randrange

# Set MAX_ARRAY_LENGTH at least to the maximum length of your data input. Then
# adapt the bitsize of this number.  The bitsize is required for the random
# function, which only takes Python native int, which therefore has to be preset.
MAX_ARRAY_LENGTH = 10000
MAX_ARRAY_LENGTH_NBIT = 14

# NULL choosen here as -123456789. Be careful and change if your dataset
# contains this number.  This is necessary since sint.Array() treats python
# None like 0 (i.e., sint(None) is same as sint(0))
NULL = -123456789

# Length of mantissa in bits
sfloat.vlen = 15
# Length of exponent in bits
sfloat.plen = 10
# Statistical security parameter for floats
sfloat.kappa = 4

### PART 1: BASICS ###
# Definitions of class to support functions later.
###
class CustVector(object):
    """
        This is a prelimiary version of a customized array so it can be
        addressed like a vector, e.g., using push_back(), pop() and length().
        This makes handling of the data, especially when reading in and
        splitting up, easier.

        Since the lengths of arrays in .mpc fines have to be predefined
        before compilation, check if MAX_ARRAY_LENGTH is long enough for dataset.
        Implementation for sint.
    """
    def __init__(self):
        self.s_vector = sint.Array(MAX_ARRAY_LENGTH)
        self.end_ptr = MemValue(-1)
        self.s_vector[0] = NULL

    # TODO: change to if_then()
    @method_block
    def pop(self, index=None):
        if index is None:
            i = self.end_ptr + 0
            self.end_ptr.write(self.end_ptr - 1)
        else:
            i = index

        element = self.s_vector[i]
        self.s_vector[i] = NULL
        return element


    # TODO: checge this to use of [] operator
    @method_block
    def get_element(self, index):
        element =  self.s_vector[index]
        return element

    # TODO: change this to [] operator
    @method_block
    def set_element(self, index, element):
        self.s_vector[index] = element
        # For debugging:
        # print_ln("element %s at %s changed!", element.reveal(), index)

    @method_block
    def push_back(self, element):
        self.end_ptr.write(self.end_ptr + 1)
        self.s_vector[self.end_ptr] = element
        self.s_vector[self.end_ptr + 1] = NULL

    @method_block
    def back(self):
        if_then(self.end_ptr == -1)
        result = sint(NULL)
        else_then()
        result = self.s_vector[self.end_ptr]
        end_if()
        return result

    # TODO: check if makes sense to protect length of vector since it has to be
    # revealed for the for loops anyway
    @method_block
    def length(self):
        return sint(self.end_ptr + 1)

    # Shows the first x elements of the vector.
    @method_block
    def show(self, no_elements):
        @for_range(no_elements)
        def _(i):
            print_ln("%s", self.s_vector[i].reveal())

def test_CustVector(vector):
    """
        Function to test the new class CustVector and the input functionality.
    """
    get_external_private_array(vector, 1005, 1)
    vector.show(20)
    print_ln("-------------")
    length = vector.length()
    element = vector.back()
    print_ln("length: %s", length.reveal())
    print_ln("element: %s", element.reveal())
    print_ln("=============")
    #
    # implement your tests here

    #
    #
    vector.show(20)
    print_ln("-------------")
    length = vector.length()
    element = vector.back()
    print_ln("length: %s", length.reveal())
    print_ln("element: %s", element.reveal())
    print_ln("=============")

###
### END OF PART 1 ###

### PART 2: DATA INPUT AND PREPARATION ###
# Functions related to read in data and preparation.
###
def get_external_private_array(cust_vector, channel, player):
    """
        Function to read external private array from file.  Change in some of the
        original src/Input_Output/-Files of SCALE_MAMBA required.  For further
        information see SCALE-MAMBA documentation.
    """
    open_channel(channel)
    condition = MemValue(cint(1))
    i = MemValue(cint(0))

    @do_while
    def _():
        input = sint.get_private_input_from(player)

        if_then((input != NULL).reveal())
        cust_vector.push_back(input)
        i.write(i + 1)

        else_then()
        condition.write(0)
        end_if()
        return condition > 0

    close_channel(channel)

def split_up(x_vector, y_vector, split):
    """
        Function to split up dataset into training and test sets.

        Actual length of train and test (considering split) can not be taken into
        account at initialization, since array lengths have to be known at
        compile time.
    """
    x_train = CustVector()
    y_train = CustVector()
    x_test = CustVector()
    y_test = CustVector()
    data_length = x_vector.length()
    y_length = y_vector.length()

    if_then((data_length != y_vector.length()).reveal())
    print_ln("X and Y data do not have the same length! Check your input!")
    end_if()

    # Calculate target lengths of train and test vectors
    train_target_length = sfloat(data_length) * sfloat(split)
    train_target_length = mpc_math.floor_fx(sfix(train_target_length)) + 1
    test_target_length = data_length - train_target_length

    # Cast train_target_length to sint(), which is required in order to work
    # with the computation of the condition delta_check of the first while-loop.
    train_target_length_sint = floatingpoint.Trunc(
        train_target_length.v, train_target_length.k - train_target_length.f,
        train_target_length.f, train_target_length.kappa)
    train_target_length_sint_array = sint.Array(1)
    train_target_length_sint_array[0] = train_target_length_sint

    # Loop to add elements of data to train set. Elements are choosen randomly.
    # Way of getting random index to pick the elements should be improved,
    # if possible.  For now, since get_random_int() only takes python native int
    # as input, a second while loop is required.  It looks for a random int in
    # the range of MAX_ARRAY_LENGTH until this random number is within the range
    # of the data set.  This is very inefficient and slow.  A posisble solution
    # might involve changing the SCALE-MAMBA source code and might be
    # implemented later.
    @do_while
    def _():
        index = MemValue(sint.get_random_int(MAX_ARRAY_LENGTH_NBIT))
        condition = MemValue(1)
        x_element = MemValue(sint(NULL))
        y_element = MemValue(sint(NULL))

        # Use count for debugging
        count = MemValue(0)

        # Loop until random number within range of data_length found and
        # the element at this place has not been assigned to the train set, yet.
        @do_while
        def _():
            # if: 1st if-block
            if_then((index < data_length).reveal())
            # TODO: check for possibility to not have to reveal index here
            x_element.write(x_vector.get_element(index.reveal()))
            y_element.write(y_vector.get_element(index.reveal()))

            # if: 2nd if-block
            # TODO could add another check on y_element for safety reasons later
            if_then((x_element != NULL).reveal())
            ind = index.reveal()
            x_vector.set_element(ind, NULL)
            y_vector.set_element(ind, NULL)
            condition.write(0)

            # elseif: 2nd if-block
            else_then()
            index.write(sint.get_random_int(MAX_ARRAY_LENGTH_NBIT))
            count.write(count + 1)

            # end: 2nd if-block
            end_if()

            # elseif: 1st if-block
            else_then()
            index.write(sint.get_random_int(MAX_ARRAY_LENGTH_NBIT))
            count.write(count + 1)

            # end: 1st if-block
            end_if()
            return condition > 0


        # print_ln("count: %s", count)
        # print_ln("index: %s",index.reveal())
        # print_ln("x_elem: %s", x_element.reveal())
        # print_ln("y_elem: %s", y_element.reveal())

        x_train.push_back(x_element)
        y_train.push_back(y_element)
        delta_check = (train_target_length_sint_array[0]
            - x_train.length()).reveal()
        print_ln("progress splitting: %s", delta_check)
        return delta_check > 0

    # Loop over data vector to assign all left elements in data to test vector.
    @for_range(data_length.reveal())
    def _(i):
        x_element = x_vector.get_element(cint(i))
        y_element = y_vector.get_element(cint(i))
        if_then((x_element != NULL).reveal())
        if_then((y_element != NULL).reveal())
        x_test.push_back(x_element)
        y_test.push_back(y_element)
        end_if()
        end_if()

    return x_train, x_test, y_train, y_test

def use_pre_split_data(x_tr, x_te, y_tr, y_te):
    """
        Hardcoded split dataset to ensure same randomization in order to compare
        performance with original algorithm implemented in python and for testing.
        Swedish insurance dataset with split of 0.7.
    """

    # X training data
    x_tr.push_back(45)
    x_tr.push_back(12)
    x_tr.push_back(30)
    x_tr.push_back(15)
    x_tr.push_back(17)
    x_tr.push_back(40)
    x_tr.push_back(3)
    x_tr.push_back(10)
    x_tr.push_back(11)
    x_tr.push_back(24)
    x_tr.push_back(6)
    x_tr.push_back(61)
    x_tr.push_back(3)
    x_tr.push_back(7)
    x_tr.push_back(24)
    x_tr.push_back(14)
    x_tr.push_back(41)
    x_tr.push_back(19)
    x_tr.push_back(5)
    x_tr.push_back(13)
    x_tr.push_back(9)
    x_tr.push_back(108)
    x_tr.push_back(55)
    x_tr.push_back(7)
    x_tr.push_back(9)
    x_tr.push_back(26)
    x_tr.push_back(11)
    x_tr.push_back(22)
    x_tr.push_back(124)
    x_tr.push_back(57)
    x_tr.push_back(23)
    x_tr.push_back(13)
    x_tr.push_back(53)
    x_tr.push_back(31)
    x_tr.push_back(20)
    x_tr.push_back(27)
    x_tr.push_back(23)
    x_tr.push_back(25)
    x_tr.push_back(29)
    x_tr.push_back(5)
    x_tr.push_back(41)
    x_tr.push_back(3)
    x_tr.push_back(37)
    x_tr.push_back(8)
    x_tr.push_back(8)

    # Y training data
    y_tr.push_back(214)
    y_tr.push_back(58)
    y_tr.push_back(195)
    y_tr.push_back(32)
    y_tr.push_back(142)
    y_tr.push_back(119)
    y_tr.push_back(4)
    y_tr.push_back(65)
    y_tr.push_back(57)
    y_tr.push_back(138)
    y_tr.push_back(15)
    y_tr.push_back(218)
    y_tr.push_back(40)
    y_tr.push_back(28)
    y_tr.push_back(135)
    y_tr.push_back(78)
    y_tr.push_back(181)
    y_tr.push_back(46)
    y_tr.push_back(40)
    y_tr.push_back(90)
    y_tr.push_back(87)
    y_tr.push_back(393)
    y_tr.push_back(163)
    y_tr.push_back(78)
    y_tr.push_back(52)
    y_tr.push_back(188)
    y_tr.push_back(24)
    y_tr.push_back(162)
    y_tr.push_back(422)
    y_tr.push_back(171)
    y_tr.push_back(57)
    y_tr.push_back(16)
    y_tr.push_back(245)
    y_tr.push_back(210)
    y_tr.push_back(98)
    y_tr.push_back(93)
    y_tr.push_back(113)
    y_tr.push_back(69)
    y_tr.push_back(133)
    y_tr.push_back(21)
    y_tr.push_back(73)
    y_tr.push_back(13)
    y_tr.push_back(153)
    y_tr.push_back(76)
    y_tr.push_back(56)

    # X test data
    x_te.push_back(48)
    x_te.push_back(23)
    x_te.push_back(7)
    x_te.push_back(2)
    x_te.push_back(6)
    x_te.push_back(6)
    x_te.push_back(9)
    x_te.push_back(29)
    x_te.push_back(4)
    x_te.push_back(4)
    x_te.push_back(0)
    x_te.push_back(4)
    x_te.push_back(16)
    x_te.push_back(60)
    x_te.push_back(11)
    x_te.push_back(13)
    x_te.push_back(13)
    x_te.push_back(14)

    # X_test data
    y_te.push_back(248)
    y_te.push_back(40)
    y_te.push_back(49)
    y_te.push_back(7)
    y_te.push_back(51)
    y_te.push_back(15)
    y_te.push_back(49)
    y_te.push_back(104)
    y_te.push_back(12)
    y_te.push_back(38)
    y_te.push_back(0)
    y_te.push_back(13)
    y_te.push_back(60)
    y_te.push_back(202)
    y_te.push_back(21)
    y_te.push_back(93)
    y_te.push_back(32)
    y_te.push_back(96)

#
### END OF PART 2 ###

### PART 3: Computation of RMSE ###
# Definitions of funtions for the computation of the rmse start here.
# Use of arrays instead of MemValue in some cases because MemValue() can not
# contain sfloat.  For debugging and evaluating performance in comparison to the
# python implemenation, some print statements are kept.
###

def sum(vector):
    """
        Calculate the sum of the elements in a vector.
    """
    result_array = sfloat.Array(1)
    result_array[0] = 0.0

    @for_range(vector.length().reveal())
    def _(i):
        result_array[0] = result_array[0] + sfloat(vector.get_element(i))
    return result_array[0]

def mean(vector):
    """
        Calculate the mean of the elements in a vector.
    """
    return sum(vector) / sfloat(vector.length())

def rmse_metric(actual, predicted_array):
    """
        Calculate root mean squared error.
    """
    sum_err = sfloat.Array(1)
    sum_err[0] = 0.0
    rmse = sfloat(0.0)
    actual_length = actual.length().reveal()

    @for_range(actual_length)
    def _(i):
        prediction_err = predicted_array[i] - sfloat(actual.get_element(i))
        # print_ln("pred_err %s", prediction_err.reveal())
        sum_err[0] = sum_err[0] + (prediction_err ** 2)

    # print_ln("sum_err %s", sum_err[0].reveal())
    # print_ln("act_len %s", sfloat(sint(actual_length)).reveal())
    mean_err = sum_err[0] / sfloat(sint(actual_length))
    # print_ln("mean_err %s", mean_err.reveal())
    rmse = mpc_math.sqrt_simplified_fx(sfix(mean_err))
    return rmse

def covariance(x_vector, x_mean, y_vector, y_mean):
    """
        Clalculate covariance between x and y.
    """
    covar = sfloat.Array(1)
    covar[0] = 0.0
    @for_range(x_vector.length().reveal())
    def _(i):
        covar[0] = covar[0] + ((sfloat(x_vector.get_element(i)) - x_mean)
            * (sfloat(y_vector.get_element(i)) - y_mean))
    return covar[0]

def variance(vector, mean):
    """
        Calculate variance between elements of a vector.
    """
    var = sfloat.Array(1)
    var[0] = 0.0
    @for_range(vector.length().reveal())
    def _(i):
        var[0] = var[0] + (sfloat(vector.get_element(i)) - mean) ** 2
    return var[0]

def coefficients(x_vector, y_vector):
    """
        Calculate coefficients b0 and b1 for the simple_linear_regression algorithm
    """
    x_mean = mean(x_vector)
    y_mean = mean(y_vector)
    b1 = (covariance(x_vector, x_mean, y_vector, y_mean)
        / variance(x_vector, x_mean))
    b0 = y_mean - b1*x_mean
    return b0, b1

def simple_linear_regression(x_train, x_test, y_train):
    """
        Simple linear regression algorithm
    """
    x_test_length = x_test.length().reveal()
    predictions = sfloat.Array(MAX_ARRAY_LENGTH)
    b0, b1 = coefficients(x_train, y_train)
    @for_range(x_test_length)
    def _(i):
        predictions[i] = b0 + b1*sfloat(x_test.get_element(i))
    return predictions

def evaluate(x_train, x_test, y_train, y_test):
    """
        Evaluate data and receive root mean square error
    """
    predicted = simple_linear_regression(x_train, x_test, y_train)
    rmse = rmse_metric(y_test, predicted)
    return rmse

def load_data_from_file(split):
    """
        Load data from an input file. Choose channels depending on your setting
        in src/Input_Output/Input_Output_<user_defined>.cpp file
    """
    # Load external private data separated into x and y
    x = CustVector()
    get_external_private_array(x, 25, 1)
    # Uncomment to check input
    # print_ln("++++++ this is x: +++++++")
    # x.show(20)
    # print_ln("+++++++++++++++++++++++++")

    y = CustVector()
    get_external_private_array(y, 35, 1)
    # Uncomment to check input
    # print_ln("++++++ this is y: +++++++")
    # y.show(20)
    # print_ln("+++++++++++++++++++++++++")

    x_train, x_test, y_train, y_test = split_up(x, y, split)
    return x_train, x_test, y_train, y_test
#
### END OF PART 3 ###

def main():
    """
    Main function to run program.
    """
    # Initialize data vectors.
    x_train = CustVector()
    x_test = CustVector()
    y_train = CustVector()
    y_test = CustVector()

    # Set split
    split = 0.7

    # Pick / uncomment which data input is desired:
    # 1. Regular loading of data set.
    x_train, x_test, y_train, y_test = load_data_from_file(split)

    # 2. Using preset data for testing & comparison with implemenation in python.
    # use_pre_split_data(x_train, x_test, y_train, y_test)

    rmse = evaluate(x_train, x_test, y_train, y_test)
    print_ln("==========================")
    print_ln("RMSE: %s", rmse.reveal())
    print_ln("==========================")

main()
